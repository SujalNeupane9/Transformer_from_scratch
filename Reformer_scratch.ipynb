{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reformer Model\n",
        "\n",
        "Reformer model i.e. Reversible Transformer is the imporved, faster version of Transformers model introduced by a team at Google Research(Nikita Kitaev, ≈Åukasz Kaiser, Anselm Levskaya). It is a Transformer model designed to handle context windows of up to 1 million words, all on a single accelerator and using only 16GB of memory. \n",
        "\n",
        "\n",
        "**References**:\n",
        "* https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html\n",
        "\n",
        "* https://arxiv.org/abs/2001.04451 \n",
        "* https://huggingface.co/blog/reformer"
      ],
      "metadata": {
        "id": "cJuVgYRlcvEe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_7XRHEi3cjku"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Reformer, each part of the standard transformer architecture is re-engineered for minimal memory requirement without a significant drop in preformance.\n",
        "\n",
        "The memory improvements is due to the new fatures introduced by the authors of the Reformer.\n",
        "\n",
        "* **Locality-Sensitive Hashing Attention:**   The dot predouct has been replaced by the locality-sensitive hashing, which changes the complexity from O(L^2) to O(L logL) where L is the length of the sequence. Locality sensitive Hashing is a method that map high dimensional vectors to a set of discrete values(buckets/clusters).\n",
        "\n",
        "* **Chunked Feed Forward layers:**   Chunking is a technique that allows to effectively trade better memory consumption for increased time consumption.\n",
        "\n",
        "* **Reversible Residual layers:**    This technique is based on ResNet and allows storing activations only once in the training process instead of N times, where N is the number of layers.\n",
        "\n",
        "* **Axial Positional Encoding:**     In the Reformer model, each word or token in the input sequence is represented as a vector. These vectors are then augmented with additional information about the position of the word in the sequence, using a technique called positional encoding. This allows the model to take into account the order of the words in the input sequence."
      ],
      "metadata": {
        "id": "qh2CEXLZgkSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSH(nn.Module):\n",
        "    def __init__(self, num_hashes, emb_dim, num_buckets):\n",
        "        super(LSH, self).__init__()\n",
        "        self.num_hashes = num_hashes\n",
        "        self.emb_dim = emb_dim\n",
        "        self.num_buckets = num_buckets\n",
        "\n",
        "        # create random projection vectors and bias terms\n",
        "        self.projections = nn.Parameter(torch.randn(num_hashes, emb_dim))\n",
        "        self.biases = nn.Parameter(torch.randn(num_hashes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # project the input vectors to the hash space\n",
        "        hashes = torch.matmul(x, self.projections.transpose(0, 1))\n",
        "        hashes += self.biases\n",
        "        hashes = torch.floor_divide(hashes, self.num_buckets)\n",
        "\n",
        "        # return the hashes as a long tensor\n",
        "        return hashes.long()"
      ],
      "metadata": {
        "id": "mvcjpYP4lnmS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AxialPositionalEmbedding(nn.Module):\n",
        "    def __init__(self, emb_dim, max_seq_len):\n",
        "        super(AxialPositionalEmbedding, self).__init__()\n",
        "        self.emb_dim = emb_dim\n",
        "        self.max_seq_len = max_seq_len\n",
        "        \n",
        "        self.row_embeddings = nn.Parameter(torch.randn(max_seq_len, emb_dim ))  \n",
        "        self.col_embeddings = nn.Parameter(torch.randn(max_seq_len, emb_dim ))  \n",
        "        \n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, emb_dim = x.shape\n",
        "        assert seq_len <= self.max_seq_len, f\"Sequence length {seq_len} exceeds maximum sequence length {self.max_seq_len}\"\n",
        "        \n",
        "        row_pos = torch.arange(seq_len , device=x.device) \n",
        "        col_pos = torch.arange(seq_len , device=x.device) \n",
        "        row_embs = self.row_embeddings[row_pos, :].unsqueeze(0).expand(batch_size, -1, -1)\n",
        "        col_embs = self.col_embeddings[col_pos, :].unsqueeze(0).expand(batch_size, -1, -1)\n",
        "        \n",
        "        if seq_len % 2 == 0:\n",
        "            x_even = x[:, 0::2, :]\n",
        "            x_odd = x[:, 1::2, :]\n",
        "            x = torch.cat([x_even @ row_embs, x_odd @ col_embs], dim=1)\n",
        "        else:\n",
        "            x_even = x[:, 0::2, :]\n",
        "            x_odd = x[:, 1::2, :]\n",
        "            x = torch.cat([x_even @ row_embs, x_odd @ col_embs, x[:, -1, :].unsqueeze(1)], dim=1)\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "pq92qNfHcvl6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReformerLayer(nn.Module):\n",
        "    def __init__(self, emb_dim, num_heads, hidden_dim, chunk_size, dropout=0.1):\n",
        "        super(ReformerLayer, self).__init__()\n",
        "        self.emb_dim = emb_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.dropout = dropout\n",
        "        self.chunk_size = chunk_size\n",
        "        \n",
        "        self.self_attn = nn.MultiheadAttention(emb_dim, num_heads, dropout=dropout)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(emb_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(hidden_dim, emb_dim),\n",
        "            nn.Dropout(p=dropout)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(emb_dim)\n",
        "        self.norm2 = nn.LayerNorm(emb_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.norm1(x)\n",
        "        # Chunk input tensor along sequence length dimension\n",
        "        chunks = x.chunk(x.size(1) // self.chunk_size, dim=1)\n",
        "        chunked_output = []\n",
        "        for chunk in chunks:\n",
        "            chunk = chunk.permute(1, 0, 2)\n",
        "            chunk, _ = self.self_attn(chunk, chunk, chunk)\n",
        "            chunk = F.dropout(chunk, p=self.dropout, training=self.training)\n",
        "            chunk = chunk.permute(1, 0, 2)\n",
        "            chunked_output.append(chunk)\n",
        "        x = torch.cat(chunked_output, dim=1)\n",
        "        x = x + residual\n",
        "        \n",
        "        residual = x\n",
        "        x = self.norm2(x)\n",
        "        # Chunk input tensor along sequence length dimension\n",
        "        chunks = x.chunk(x.size(1) // self.chunk_size, dim=1)\n",
        "        chunked_output = []\n",
        "        for chunk in chunks:\n",
        "            chunked_output.append(self.feed_forward(chunk))\n",
        "        x = torch.cat(chunked_output, dim=1)\n",
        "        x = x + residual\n",
        "        return x"
      ],
      "metadata": {
        "id": "KPGbEFjQdTXS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Reformer(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, num_heads, hidden_dim, num_layers, max_seq_len, num_hashes, num_buckets,chunk_size, dropout=0.1):\n",
        "        super(Reformer, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.emb_dim = emb_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.num_hashes = num_hashes\n",
        "        self.num_buckets = num_buckets\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.token_emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.axial_position_emb = AxialPositionalEmbedding(emb_dim, max_seq_len)\n",
        "        self.lsh = LSH(num_hashes, emb_dim, num_buckets)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            ReformerLayer(emb_dim, num_heads, hidden_dim,chunk_size, dropout=dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.fc = nn.Linear(emb_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encode the input sequence\n",
        "        x = self.token_emb(x)\n",
        "        x = self.axial_position_emb(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # generate LSH hashes for the input sequence\n",
        "        hashes = self.lsh(x.view(-1, self.emb_dim))\n",
        "        hashes = hashes.view(x.size(0), x.size(1), self.num_hashes)\n",
        "\n",
        "        # apply the transformer layers\n",
        "        for layer in self.layers:\n",
        "            residual = x\n",
        "            x = layer(x)\n",
        "            x += residual\n",
        "\n",
        "        # compute the logits and return the output\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "6x4C9e--l1S5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10000\n",
        "emb_dim = 512\n",
        "num_heads = 8\n",
        "hidden_dim = 2048\n",
        "num_layers = 12\n",
        "max_seq_len = 512\n",
        "num_hashes = 6\n",
        "num_buckets = 8\n",
        "chunk_size = 64\n",
        "\n",
        "model = Reformer(vocab_size, emb_dim, num_heads, hidden_dim, num_layers, max_seq_len, num_hashes, num_buckets, chunk_size)\n",
        "\n",
        "input_seq = torch.randint(low=0, high=vocab_size, size=(4, 512))\n",
        "\n",
        "output = model(input_seq)\n",
        "print(output.shape) # should be torch.Size([4, 512, 10000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNCFaP5fdgyk",
        "outputId": "5d3c3a75-a1cc-48f3-baae-1b840b08ac0f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 512, 10000])\n"
          ]
        }
      ]
    }
  ]
}