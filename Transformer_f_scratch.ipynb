{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Here I've attempted to build a Transformer from scratch taking reference from the article  https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb and the research paper https://arxiv.org/abs/1706.03762\n",
        "\n",
        "Let me walk you through everything I've learned from these references."
      ],
      "metadata": {
        "id": "XSBaR7bntrpc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll start by importing all the necessary libraries. "
      ],
      "metadata": {
        "id": "NN9xE9fb6Oj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import copy"
      ],
      "metadata": {
        "id": "Xoo1ybsvst-N"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Architecture of Transformer model\n",
        "\n",
        "The transformer model uses a self-attention mechanisms, which allows it to consider all the previous words in a sequence when processing the next word. This is in contrast to traditional RNNs, which processes one word at a time in a sequential manner. The self-attention mechanism allows the Transformer to capture long-term sequences more efficiently.\n",
        " \n",
        "\n",
        "The Transformer model consists of an encoder and a decoder. The encoder takes an input sequence and produces a sequence of hidden states, while the decoder takes the encoder output and generates an output sequence. Both the encoder and decoder are made up of multiple layers, each of which includes a self-attention mechanism and a feed-forward neural network."
      ],
      "metadata": {
        "id": "RpqcxXnW687_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Multi-Head Attention\n",
        "\n",
        "Multi-Head Attention is a key component of the Transformer model. The idea behind it is to compute the attention mechanism multiple times in parallel, with different sets of weights, in order to allow the model to attend to different parts of the input representation simultaneously."
      ],
      "metadata": {
        "id": "RUP0FU3U6Z8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Multi-Head Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self,d_model,num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads ==0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model ,d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q,K,V, mask=None):\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2,-1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0,-1e9)\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1,2)\n",
        "\n",
        "    def combine_heads(self,x):\n",
        "        batch_size, _, seq_length, d_k = x.size()\n",
        "        return x.transpose(1,2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "    def forward(self, Q,K,V,mask=None):\n",
        "        Q = self.split_heads(self.W_q(Q))\n",
        "        K = self.split_heads(self.W_k(K))\n",
        "        V = self.split_heads(self.W_v(V))\n",
        "\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V,mask)\n",
        "        output = self.W_o(self.combine_heads(attn_output))\n",
        "        return output"
      ],
      "metadata": {
        "id": "ujxJ2CzLszvb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Position-wise Feed Forward Networks\n",
        "\n",
        "In a Transformer model, a position-wise feed-forward network (FFN) is applied to each position of the encoder and decoder. It consists of two linear transformations with a ReLU activation in between them:\n",
        "FFN(x) = max(0, xW1 + b1)W2 + b2 \n",
        "where x is the input tensor of shape (seq_len, embed_dim), W1, W2 are the learnable weight matrices of the two linear transformations, and b1, b2 are the bias terms. The output of the FFN is of the same shape as the input."
      ],
      "metadata": {
        "id": "Ze1s1eEu_gDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Position-wise Feed Forward Networks\n",
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForward,self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))"
      ],
      "metadata": {
        "id": "Dw1PcMexszp3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Positional Encoding\n",
        "\n",
        "Self-attention mechanism operates on the input sequence as a whole, rather than individual words or phrases. The self-attention mechanism calculates a weight for each input token based on its similarity to all other tokens in the sequence. This means that the output of self-attention is a weighted sum of all input tokens, where each weight is based on the similarity between the corresponding token and all other tokens in the sequence. *So the self-attention mechanism is not sensitive to word ordering.*\n",
        "\n",
        "**Does it matter?**\n",
        "Let us consider an example:\n",
        "* All humans are smart and some are dumb.\n",
        "* All humans are dumb and some are smart.\n",
        "\n",
        "Does these two sentences have same meaning?  (Of course not!)\n",
        "\n",
        "The solution to this problem is to add a position embedding to the input word embedding. We can encode absolute and relative positions of the words such that the semantic of the input is not altered. It is of same dimension as the word embedding."
      ],
      "metadata": {
        "id": "bBrjN1Uy8zI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Positional encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,d_model,max_seq_length):\n",
        "        super(PositionalEncoding,self).__init__()\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0,d_model,2).float() * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:,0::2] = torch.sin(position*div_term)\n",
        "        pe[:,1::2] = torch.cos(position*div_term)\n",
        "\n",
        "        self.register_buffer('pe',pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self,x):\n",
        "        return x+self.pe[:,:x.size(1)]"
      ],
      "metadata": {
        "id": "yOdaLyidszmU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. a) Encoder Layer\n",
        "\n",
        "The encoder layer is a core building block of the Transformer model used in natural language processing tasks. It consists of two main components: multi-head self-attention mechanism and position-wise feedforward network.\n",
        "\n",
        "The Transformer model typically stacks multiple encoder layers on top of each other, allowing the model to capture increasingly complex relationships between the input sequence elements. The output of the final encoder layer is then fed to the decoder layer to generate the final output sequence."
      ],
      "metadata": {
        "id": "sG26QZf4_3L_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Encoder layer\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self,d_model,num_heads,d_ff,dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model,num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,x,mask):\n",
        "        attn_output = self.self_attn(x,x,x,mask)\n",
        "        x = self.norm1(x+self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x+self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "jz8rPVkqszic"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 b) Decoder Layer\n",
        "\n",
        "The decoder layer of a transformer model is similar to the encoder layer, but with some differences to enable it to perform the task of language generation. The decoder layer has three sub-layers: masked multi-head attention, multi-head attention, and position-wise feedforward network.\n",
        "\n",
        "The masked multi-head attention layer is similar to the self-attention layer in the encoder, but with a mask to prevent the decoder from attending to future tokens. This mask ensures that at each step, the decoder attends only to the tokens generated in the previous steps.\n",
        "\n",
        "The second sub-layer is the multi-head attention layer, where the decoder attends to the encoder's output to obtain context information for the current step. This attention mechanism allows the decoder to focus on the relevant parts of the encoder output and suppress irrelevant information.\n",
        "\n",
        "The final sub-layer of the decoder layer is the position-wise feedforward network, which is similar to the one in the encoder layer. This network applies a two-layer linear transformation followed by a ReLU activation to each position in the sequence independently."
      ],
      "metadata": {
        "id": "TLTjx1GbAcsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Decoder Layer\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self,d_model,num_heads,d_ff,dropout):\n",
        "        super(DecoderLayer,self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionalEncoding(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self,x,enc_output,src_mask,tgt_mask):\n",
        "        attn_output = self.self_attn(x,x,x,tgt_mask)\n",
        "        x = self.norm1(x+self.dropout(attn_output))\n",
        "        attn_output = self.cross_attn(x,enc_output,enc_output,src_mask)\n",
        "        x = self.norm2(x+self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x+self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "3GI9sC_qtHGg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Transformer Architecture"
      ],
      "metadata": {
        "id": "7i8sDVJaAvPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##  Transformer Model\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self,src_vocab_size,tgt_vocab_size,d_model,num_heads,num_layers,d_ff,max_seq_length,dropout):\n",
        "        super(Transformer,self).__init__()\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size,d_model)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size,d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model,max_seq_length)\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model,num_heads,d_ff,dropout) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model,num_heads,d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self,src,tgt):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)\n",
        "        seq_length = tgt.size(1)\n",
        "        nopeak_mask = (1-torch.triu(torch.ones(1,seq_length,seq_length),diagonal=1)).bool()\n",
        "        tgt_mask = tgt_mask & nopeak_mask\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self,src,tgt):\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
        "\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output,src_mask)\n",
        "        \n",
        "        dec_output = tgt_embedded\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output,enc_output,src_mask,tgt_mask)\n",
        "\n",
        "        output = self.fc(dec_output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "GscDwf6stG5r"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the model on sample data"
      ],
      "metadata": {
        "id": "8wk4DuG8BDVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab_size = 5000\n",
        "tgt_vocab_size = 5000\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "max_seq_length = 100\n",
        "dropout = 0.1"
      ],
      "metadata": {
        "id": "n8WqwlgHtGyt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(src_vocab_size, tgt_vocab_size,d_model,num_heads,num_layers,d_ff,max_seq_length,dropout)"
      ],
      "metadata": {
        "id": "49l4ib-9tdpv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3hOFsUgCGWrD"
      },
      "outputs": [],
      "source": [
        "## Generate some random data\n",
        "src_data = torch.randint(1,src_vocab_size, (64,max_seq_length)) #(batch_size,seq_length)\n",
        "tgt_data = torch.randint(1,tgt_vocab_size, (64,max_seq_length)) #(batch_size,seq_length)\n",
        "\n",
        "## Train the model\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(transformer.parameters(),lr=0.0001,betas=(0.9,0.98),eps=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "K15_Zfl5Gcbj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2467f4a8-4f9a-4391-ca71-9ce542091705"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:1, Loss:8.677156448364258\n",
            "Epoch:2, Loss:8.606093406677246\n",
            "Epoch:3, Loss:8.556018829345703\n",
            "Epoch:4, Loss:8.513888359069824\n",
            "Epoch:5, Loss:8.479804039001465\n",
            "Epoch:6, Loss:8.448040008544922\n",
            "Epoch:7, Loss:8.418245315551758\n",
            "Epoch:8, Loss:8.39105224609375\n",
            "Epoch:9, Loss:8.367931365966797\n",
            "Epoch:10, Loss:8.345005989074707\n",
            "Epoch:11, Loss:8.31788158416748\n",
            "Epoch:12, Loss:8.290118217468262\n",
            "Epoch:13, Loss:8.266304016113281\n",
            "Epoch:14, Loss:8.239670753479004\n",
            "Epoch:15, Loss:8.215306282043457\n",
            "Epoch:16, Loss:8.186251640319824\n",
            "Epoch:17, Loss:8.159092903137207\n",
            "Epoch:18, Loss:8.132688522338867\n",
            "Epoch:19, Loss:8.105985641479492\n",
            "Epoch:20, Loss:8.077210426330566\n",
            "Epoch:21, Loss:8.049870491027832\n",
            "Epoch:22, Loss:8.021150588989258\n",
            "Epoch:23, Loss:7.990198135375977\n",
            "Epoch:24, Loss:7.961597919464111\n",
            "Epoch:25, Loss:7.932947158813477\n",
            "Epoch:26, Loss:7.90463924407959\n",
            "Epoch:27, Loss:7.871876239776611\n",
            "Epoch:28, Loss:7.840559959411621\n",
            "Epoch:29, Loss:7.809232234954834\n",
            "Epoch:30, Loss:7.779618263244629\n",
            "Epoch:31, Loss:7.7461419105529785\n",
            "Epoch:32, Loss:7.713969707489014\n",
            "Epoch:33, Loss:7.682716369628906\n",
            "Epoch:34, Loss:7.642611503601074\n",
            "Epoch:35, Loss:7.6113176345825195\n",
            "Epoch:36, Loss:7.574446678161621\n",
            "Epoch:37, Loss:7.5392537117004395\n",
            "Epoch:38, Loss:7.5081915855407715\n",
            "Epoch:39, Loss:7.477425575256348\n",
            "Epoch:40, Loss:7.442707061767578\n",
            "Epoch:41, Loss:7.4031147956848145\n",
            "Epoch:42, Loss:7.3613810539245605\n",
            "Epoch:43, Loss:7.326648235321045\n",
            "Epoch:44, Loss:7.288286209106445\n",
            "Epoch:45, Loss:7.2582221031188965\n",
            "Epoch:46, Loss:7.224493980407715\n",
            "Epoch:47, Loss:7.188015937805176\n",
            "Epoch:48, Loss:7.154922008514404\n",
            "Epoch:49, Loss:7.11444091796875\n",
            "Epoch:50, Loss:7.0875420570373535\n",
            "Epoch:51, Loss:7.044629096984863\n",
            "Epoch:52, Loss:7.014560699462891\n",
            "Epoch:53, Loss:6.97859525680542\n",
            "Epoch:54, Loss:6.944268226623535\n",
            "Epoch:55, Loss:6.908255577087402\n",
            "Epoch:56, Loss:6.877530574798584\n",
            "Epoch:57, Loss:6.843665599822998\n",
            "Epoch:58, Loss:6.814271926879883\n",
            "Epoch:59, Loss:6.777859687805176\n",
            "Epoch:60, Loss:6.743403434753418\n",
            "Epoch:61, Loss:6.7100396156311035\n",
            "Epoch:62, Loss:6.676053047180176\n",
            "Epoch:63, Loss:6.64457368850708\n",
            "Epoch:64, Loss:6.608253002166748\n",
            "Epoch:65, Loss:6.581057071685791\n",
            "Epoch:66, Loss:6.54963493347168\n",
            "Epoch:67, Loss:6.515414714813232\n",
            "Epoch:68, Loss:6.47689962387085\n",
            "Epoch:69, Loss:6.4534382820129395\n",
            "Epoch:70, Loss:6.4177937507629395\n",
            "Epoch:71, Loss:6.388492584228516\n",
            "Epoch:72, Loss:6.3535943031311035\n",
            "Epoch:73, Loss:6.323247909545898\n",
            "Epoch:74, Loss:6.287760257720947\n",
            "Epoch:75, Loss:6.257987022399902\n",
            "Epoch:76, Loss:6.2269287109375\n",
            "Epoch:77, Loss:6.197871208190918\n",
            "Epoch:78, Loss:6.169437885284424\n",
            "Epoch:79, Loss:6.1315178871154785\n",
            "Epoch:80, Loss:6.102378845214844\n",
            "Epoch:81, Loss:6.077169895172119\n",
            "Epoch:82, Loss:6.049164772033691\n",
            "Epoch:83, Loss:6.01666259765625\n",
            "Epoch:84, Loss:5.984189987182617\n",
            "Epoch:85, Loss:5.944676876068115\n",
            "Epoch:86, Loss:5.9260029792785645\n",
            "Epoch:87, Loss:5.8938822746276855\n",
            "Epoch:88, Loss:5.8586578369140625\n",
            "Epoch:89, Loss:5.831420421600342\n",
            "Epoch:90, Loss:5.800144195556641\n",
            "Epoch:91, Loss:5.770647048950195\n",
            "Epoch:92, Loss:5.7426438331604\n",
            "Epoch:93, Loss:5.712627410888672\n",
            "Epoch:94, Loss:5.680867671966553\n",
            "Epoch:95, Loss:5.646454811096191\n",
            "Epoch:96, Loss:5.617368221282959\n",
            "Epoch:97, Loss:5.59437370300293\n",
            "Epoch:98, Loss:5.558202266693115\n",
            "Epoch:99, Loss:5.528753280639648\n",
            "Epoch:100, Loss:5.500906944274902\n"
          ]
        }
      ],
      "source": [
        "transformer.train()\n",
        "\n",
        "## training Loop\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    output = transformer(src_data,tgt_data[:,:-1])\n",
        "    loss = criterion(output.contiguous().view(-1,tgt_vocab_size),tgt_data[:,1:].contiguous().view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch:{epoch+1}, Loss:{loss.item()}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}